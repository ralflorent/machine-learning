{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Machine Learning - Mini Project II\n",
    "\n",
    "### Learning Algorithm: Classifier\n",
    "\n",
    "Created on March 10, 2019 by Diogo Cosin <d.ayresdeoliveira@jacobs-university.de> and Ralph Florent <r.florent@jacobs-university.de>.\n",
    "\n",
    "### Description\n",
    "Train a classifier for the Digits dataset by implementing a full processing pipeline from feature extraction to (linear) classifier training, attempting to squeeze performance out of the classifier using cross-validation and regularization techniques.\n",
    "\n",
    "### Summary\n",
    "The script below is intended to... \n",
    "\n",
    "WIP\n",
    "\n",
    "Note: The algorithm is tested on the OCR datasets from the `DigitsBasicsRoutine.zip`, which was provided by Professor Dr. H. Jaeger, Machine Learning Professor at [Jacobs University Bremen](https://www.jacobs-university.de)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Learning Algorithm: Classifier \"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# \n",
    "# Created on April 01, 2019\n",
    "# Authors: \n",
    "#        Diogo Cosin <d.ayresdeoliveira@jacobs-university.de>,\n",
    "#        Ralph Florent <r.florent@jacobs-university.de>\n",
    "\n",
    "\n",
    "# Import relevant libraries\n",
    "import sys\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "sys.path.append('./assets/')\n",
    "from miniprojectone import get_data_points, k_means, get_codebooks, assign_randomly\n",
    "\n",
    "TOTAL_DIGIT_CLASSES = 10\n",
    "\n",
    "# START: Helper functions\n",
    "def load_data(one_hot_encoding=None):\n",
    "    \"\"\" Load the data and parse (if specified, one-hot encoding) numbered \n",
    "        class labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        one_hot_encoding: bool, None\n",
    "            determine whether one-hot encoding should be used or not\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dataframe: array-like (n_samples, m_features)\n",
    "    \"\"\"\n",
    "    # load data frame with no class labels defined\n",
    "    dataframe = get_data_points()\n",
    "    if one_hot_encoding is None:\n",
    "        return dataframe\n",
    "    return inject_label(dataframe, one_hot_encoding)\n",
    "\n",
    "def one_hot_encode(ith, k=TOTAL_DIGIT_CLASSES):\n",
    "    \"\"\" Apply one-hot encoding technique for a k-dimensional vector\n",
    "        \n",
    "        This function is intended to work like a lightweight version one-hot-encoder.\n",
    "        It is adapted to our specific needs. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ith: int\n",
    "            ith position of the one-of-K discrete label\n",
    "        \n",
    "        k: int\n",
    "            number of k-classes for the encoding vector\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        encoded: array\n",
    "            vector with zeros and one in the ith position\n",
    "    \"\"\"\n",
    "    if ith > k:\n",
    "        ith = k # avoid out of bound exception\n",
    "        \n",
    "    index = ith - 1\n",
    "    encoded = np.zeros(k)\n",
    "    encoded[index] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def inject_label(dataset, one_hot_encoding=False):\n",
    "    \"\"\" Inject into data frame class labels as numbered or one-hot encoded \n",
    "    \"\"\"\n",
    "    dataframe, k_class = [], TOTAL_DIGIT_CLASSES\n",
    "    digits = np.array_split(dataset, k_class) # split into 10 arrays\n",
    "    for i in range(k_class):\n",
    "        digit =  digits[i] # n-obs x k-dim\n",
    "        if i == 0: \n",
    "            i = 10 # define \"0\" as class 10\n",
    "        encoded = one_hot_encode(i) if one_hot_encoding else i\n",
    "        \n",
    "        for point in digit:\n",
    "            dataframe.append( np.insert(point, len(point), encoded) )\n",
    "            \n",
    "    return np.array(dataframe) \n",
    "\n",
    "\n",
    "def split_data(dataframe):\n",
    "    \"\"\" Split data frame into training and testing data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: array-like (n_samples, m_features)\n",
    "        2000 digit patterns x 240 features and 10 one-hot encoded class labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoded: list\n",
    "        list containing array of training and array of testing data\n",
    "    \"\"\"\n",
    "    digits = np.array_split(dataframe, 10)\n",
    "    \n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    for digit in digits:\n",
    "        train_data.extend(digit[:100])\n",
    "        test_data.extend(digit[100:])\n",
    "    \n",
    "    return [np.array(train_data), np.array(test_data)]\n",
    "\n",
    "\n",
    "def select_features(dataset, codebooks):\n",
    "    \"\"\" Apply Euclidean distance between k-means centroid and each point\n",
    "    to extract k features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # transfrom xi -> fi by reducing dimensionality from n to k features\n",
    "    for point in dataset:\n",
    "        # compute distance between data point and centroid \n",
    "        feature = [np.linalg.norm(point - c) for c in codebooks]\n",
    "        feature.append(1) # padded with a trailing 1 \n",
    "        features.append( np.array(feature) )\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def compute_linreg(Phi, Zi, alpha=0):\n",
    "    \"\"\" Algorithm to compute weights of the matrix W that solves the linear regression\n",
    "    \n",
    "    Data: Given m-dimensional feature vectors (fi) and k-dimensional target vectors (zi)\n",
    "    Result: a kxm-dimensional weight matrix Wopt that solves the linear regression\n",
    "    \n",
    "    1) Sort the feature vectors row-wise into an Nxm matrix (Phi) and the targets into an\n",
    "    Nxk matrix (Z)\n",
    "    2) Compute the pseudo inverse of Phi x Zi as the Wopt transposed.\n",
    "    \n",
    "    The pseudo-inverse of a matrix A, denoted A^+, is defined as: “the matrix that ‘solves’ \n",
    "    [the least-squares problem] Ax = b,” i.e., if x is said solution, then A^+ is \n",
    "    that matrix such that x = A^+b.\n",
    "    \n",
    "    ref: \n",
    "        https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html\n",
    "        https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html\n",
    "        https://docs.scipy.org/doc/numpy/reference/generated/numpy.identity.html\n",
    "    \"\"\"\n",
    "    # compute rigde regression factor \n",
    "    ridge_factor = alpha * np.identity(Phi.shape[1]) # Imxm identity matrix\n",
    "    # compute pseudo inverse of Phi x Zi\n",
    "    Wopt = np.linalg.inv(np.dot(Phi.T, Phi) + ridge_factor).dot(Phi.T).dot(Zi)\n",
    "    return Wopt.T\n",
    "\n",
    "\n",
    "def check_accuracy(Y_ground, Y_pred):\n",
    "    # Y_ground and Y_pred are N by k matrices\n",
    "    \n",
    "    ground_labels = np.argmax(Y_ground, axis=1) # N-dimensional vector with the ground labels\n",
    "    predicted_labels = np.argmax(Y_pred, axis=1) # N-dimensional vector with the highest probability position\n",
    "    \n",
    "    correct_classifications = sum(predicted_labels == ground_labels)\n",
    "    accuracy = float(correct_classifications)/Y_ground.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def check_MSE(Y_ground, Y_pred):\n",
    "    # Y_ground and Y_pred are N by k matrices\n",
    "    \n",
    "    Y_norm = np.linalg.norm(Y_ground - Y_pred, axis=1) # N-dimensional vector with the norms\n",
    "    MSE = np.square(Y_norm).mean()\n",
    "    \n",
    "    return MSE\n",
    "\n",
    "\n",
    "def predict_hypothesis_matrix(model, X):\n",
    "    hypothesis_matrix = model@X.T # result in a k by N matrix\n",
    "    return hypothesis_matrix.T # return N by k hypothesis matrix\n",
    "\n",
    "\n",
    "def k_fold_cv(dataset, nbr_folds=5, k=10, alpha=0.0):\n",
    "    # copy the data and shuffle it\n",
    "    data = dataset.copy()\n",
    "    \n",
    "    # divide into k-groups\n",
    "    folds = assign_randomly(data, nbr_folds)\n",
    "    \n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    MSE_train, MSE_test = [], []\n",
    "    \n",
    "    # train models for the folds\n",
    "    for i in range(nbr_folds):\n",
    "#         print(\"Processing fold {}\".format(i))\n",
    "        \n",
    "        # distribute train and test data\n",
    "        test_data, train_data = folds[i], []\n",
    "        for j in range(nbr_folds):\n",
    "            if i != j:\n",
    "                train_data.extend(folds[j])\n",
    "        \n",
    "        train_data = np.array(train_data)\n",
    "        test_data = np.array(test_data)\n",
    "                \n",
    "        # separate X (inputs) and Y (labels) for training and testing data\n",
    "        x_train, y_train = train_data[:, :-TOTAL_DIGIT_CLASSES], train_data[:, -TOTAL_DIGIT_CLASSES:]\n",
    "        x_test, y_test = test_data[:, :-TOTAL_DIGIT_CLASSES], test_data[:, -TOTAL_DIGIT_CLASSES:]\n",
    "        \n",
    "        # K-mean dimensionality reduction\n",
    "        start = timer()\n",
    "        clusters = k_means(x_train, k)\n",
    "        codebooks = get_codebooks(clusters)\n",
    "        end = timer()\n",
    "        print(\"--- K-means elapsed time: {:1.10f}\".format(end - start))\n",
    "        start = timer()\n",
    "        x_train = select_features(x_train, codebooks)\n",
    "        x_test = select_features(x_test, codebooks)\n",
    "        end = timer()\n",
    "        print(\"--- Feature extraction elapsed time: {:1.10f}\".format(end - start))\n",
    "        \n",
    "        # compute models for training data\n",
    "        model = compute_linreg(x_train, y_train, alpha)\n",
    "        \n",
    "        # prediction on training and testing data\n",
    "        pred_train = predict_hypothesis_matrix(model, x_train)\n",
    "        pred_test = predict_hypothesis_matrix(model, x_test) \n",
    "        \n",
    "        # assess accuracy\n",
    "        _acc_train = check_accuracy(pred_train, y_train)\n",
    "        _acc_test  = check_accuracy(pred_test, y_test)\n",
    "        \n",
    "        accuracy_train.append(_acc_train)\n",
    "        accuracy_test.append(_acc_test)\n",
    "\n",
    "        # assesss MSE\n",
    "        _MSE_train = check_MSE(pred_train, y_train)\n",
    "        _MSE_test = check_MSE(pred_test, y_test)\n",
    "        \n",
    "        MSE_train.append(_MSE_train)\n",
    "        MSE_test.append(_MSE_test)\n",
    "        \n",
    "        cv_accuracy_train = np.mean(accuracy_train)\n",
    "        cv_accuracy_test = np.mean(accuracy_test)\n",
    "        cv_MSE_train = np.mean(MSE_train)\n",
    "        cv_MSE_test = np.mean(MSE_test)\n",
    "        \n",
    "    return cv_accuracy_train, cv_accuracy_test, cv_MSE_train, cv_MSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training and testing data\n",
    "The `load_data` function helps framing the data with numbered or one-hot encoding class labels to build an in-memory data frame. \n",
    "\n",
    "Given this data frame, let's load the first 100 digit-patterns as training data and the second 100 as testing data, for a specific digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 250) (1000, 250) (1000, 250)\n"
     ]
    }
   ],
   "source": [
    "dataframe = load_data(one_hot_encoding=True)\n",
    "\n",
    "# Distribute dataframe into training and testing data\n",
    "training_data, testing_data = split_data(dataframe)\n",
    "\n",
    "print(dataframe.shape, training_data.shape, testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Let's use K-means clustering algorithm to select relevant features in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 240) (1000, 11)\n"
     ]
    }
   ],
   "source": [
    "raw_dataframe = load_data()\n",
    "raw_training_data, raw_testing_data = split_data(raw_dataframe)\n",
    "\n",
    "clusters = k_means(raw_training_data, 10)\n",
    "codebooks = get_codebooks(clusters)\n",
    "selected_features = select_features(raw_training_data, codebooks)\n",
    "\n",
    "print(raw_training_data.shape, selected_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inject now the one hot encoded labels in the reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_labels = inject_label(selected_features, one_hot_encoding=True)\n",
    "features_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier: Linear Regression\n",
    "\n",
    "Let's compute the linear regression weights with a trailing 1 as bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi = features_labels[:,:-TOTAL_DIGIT_CLASSES]\n",
    "Zi = features_labels[:,-TOTAL_DIGIT_CLASSES:]\n",
    "weights = compute_linreg(Phi, Zi)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the accuracy and MSE through 10-fold cross-validation. In this case, we need reload the data, but now with one hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe = load_data(one_hot_encoding=True)\n",
    "raw_training_data, raw_testing_data = split_data(raw_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 0\n",
      "--- K-means elapsed time: 12.2034717980\n",
      "--- Feature extraction elapsed time: 0.1785829620\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 14.0005034330\n",
      "--- Feature extraction elapsed time: 0.1811089210\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 14.9120720690\n",
      "--- Feature extraction elapsed time: 0.1804820530\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 10.6060940190\n",
      "--- Feature extraction elapsed time: 0.1773576660\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 13.6522932980\n",
      "--- Feature extraction elapsed time: 0.1755237510\n"
     ]
    }
   ],
   "source": [
    "nbr_folds = 5\n",
    "k = 20\n",
    "alpha = 0.0\n",
    "\n",
    "cv_accuracy_train, cv_accuracy_test, cv_MSE_train, cv_MSE_test = k_fold_cv(raw_training_data, nbr_folds, k, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy is 94.45%\n",
      "The test accuracy is 92.70%\n",
      "The train MSE is 0.2290\n",
      "The test MSE is 0.2517\n"
     ]
    }
   ],
   "source": [
    "print(\"The train accuracy is {:1.2f}%\".format(100*cv_accuracy_train))\n",
    "print(\"The test accuracy is {:1.2f}%\".format(100*cv_accuracy_test))\n",
    "print(\"The train MSE is {:1.4f}\".format(cv_MSE_train))\n",
    "print(\"The test MSE is {:1.4f}\".format(cv_MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searching Alpha and K Optimal Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try different combinations of alpha and number of K-means clusters, checking the metrics for each combination. The objective here is to optimize the classifier improving, this way, the performance metrics.\n",
    "\n",
    "The results are stored in the dictionary `grid_search_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.arange(0.0, 1.01, 0.1)\n",
    "k_list = [10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.0]\n",
    "k_list = [2, 5, 10, 20, 50, 100, 200, 400, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processing k=2\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 3.6661977560\n",
      "--- Feature extraction elapsed time: 0.0296250910\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 4.8218664210\n",
      "--- Feature extraction elapsed time: 0.0402324230\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 4.9351109660\n",
      "--- Feature extraction elapsed time: 0.0401054670\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 10.2019201340\n",
      "--- Feature extraction elapsed time: 0.0407860550\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 4.1424964850\n",
      "--- Feature extraction elapsed time: 0.0378704560\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 3.5853394060\n",
      "--- Feature extraction elapsed time: 0.0400042550\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 3.1725882070\n",
      "--- Feature extraction elapsed time: 0.0396977290\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 4.3883294160\n",
      "--- Feature extraction elapsed time: 0.0402389380\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 6.3378127710\n",
      "--- Feature extraction elapsed time: 0.0368031900\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 9.8936666240\n",
      "--- Feature extraction elapsed time: 0.0371424690\n",
      "=> Processing k=5\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 9.4380907920\n",
      "--- Feature extraction elapsed time: 0.0793275580\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 8.9973522840\n",
      "--- Feature extraction elapsed time: 0.0512779190\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 4.8479821780\n",
      "--- Feature extraction elapsed time: 0.0835525320\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 9.8350973190\n",
      "--- Feature extraction elapsed time: 0.0899554350\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 6.0784070620\n",
      "--- Feature extraction elapsed time: 0.0516292780\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 8.0441155680\n",
      "--- Feature extraction elapsed time: 0.0587028640\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 10.1382861680\n",
      "--- Feature extraction elapsed time: 0.0522355080\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 11.4749799300\n",
      "--- Feature extraction elapsed time: 0.0538743720\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 19.1982291100\n",
      "--- Feature extraction elapsed time: 0.0513018640\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 8.9542121430\n",
      "--- Feature extraction elapsed time: 0.0506953940\n",
      "=> Processing k=10\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 23.7640118830\n",
      "--- Feature extraction elapsed time: 0.0936442700\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 11.8240204430\n",
      "--- Feature extraction elapsed time: 0.0928824160\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 7.1229720350\n",
      "--- Feature extraction elapsed time: 0.1043676620\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 9.9736751010\n",
      "--- Feature extraction elapsed time: 0.0978065350\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 9.9912223580\n",
      "--- Feature extraction elapsed time: 0.0956799340\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 6.6006171080\n",
      "--- Feature extraction elapsed time: 0.0933518810\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 12.4066962260\n",
      "--- Feature extraction elapsed time: 0.1032432190\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 19.3819491480\n",
      "--- Feature extraction elapsed time: 0.1639841650\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 9.3413432810\n",
      "--- Feature extraction elapsed time: 0.0942349180\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 9.4290305060\n",
      "--- Feature extraction elapsed time: 0.1017026280\n",
      "=> Processing k=20\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 10.5802011770\n",
      "--- Feature extraction elapsed time: 0.1717457170\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 13.1117392150\n",
      "--- Feature extraction elapsed time: 0.3712041700\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 14.3422684690\n",
      "--- Feature extraction elapsed time: 0.1678354840\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 16.5821593720\n",
      "--- Feature extraction elapsed time: 0.1867604080\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 23.5215543940\n",
      "--- Feature extraction elapsed time: 0.1804319070\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 17.9543423390\n",
      "--- Feature extraction elapsed time: 0.2919121780\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 19.0807965490\n",
      "--- Feature extraction elapsed time: 0.1752478060\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 15.0323119020\n",
      "--- Feature extraction elapsed time: 0.1871988000\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 15.3654001140\n",
      "--- Feature extraction elapsed time: 0.1798678710\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 13.1677186700\n",
      "--- Feature extraction elapsed time: 0.2290101410\n",
      "=> Processing k=50\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 21.6319644610\n",
      "--- Feature extraction elapsed time: 0.4691906870\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 27.9050766710\n",
      "--- Feature extraction elapsed time: 0.7276298760\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 23.7090237360\n",
      "--- Feature extraction elapsed time: 0.4011268670\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 22.5356910230\n",
      "--- Feature extraction elapsed time: 0.4175694460\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 25.1714448900\n",
      "--- Feature extraction elapsed time: 0.4059749490\n",
      "Processing fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- K-means elapsed time: 19.5398337420\n",
      "--- Feature extraction elapsed time: 0.3625948350\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 25.4884283120\n",
      "--- Feature extraction elapsed time: 0.4249371170\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 19.8938471070\n",
      "--- Feature extraction elapsed time: 0.5017061660\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 28.2417788210\n",
      "--- Feature extraction elapsed time: 0.4322608820\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 13.4851431900\n",
      "--- Feature extraction elapsed time: 0.4128898300\n",
      "=> Processing k=100\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 25.5268676080\n",
      "--- Feature extraction elapsed time: 0.7289269280\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 27.3092490770\n",
      "--- Feature extraction elapsed time: 0.7781772930\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 36.0151101330\n",
      "--- Feature extraction elapsed time: 1.0685135580\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 37.6765251140\n",
      "--- Feature extraction elapsed time: 0.7388889260\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 22.9593127740\n",
      "--- Feature extraction elapsed time: 0.8144283720\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 26.9597384200\n",
      "--- Feature extraction elapsed time: 0.6848281600\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 33.6505166520\n",
      "--- Feature extraction elapsed time: 0.8535658410\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 27.3028749490\n",
      "--- Feature extraction elapsed time: 0.7083427040\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 28.9789120740\n",
      "--- Feature extraction elapsed time: 0.7552543820\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 23.5516811650\n",
      "--- Feature extraction elapsed time: 0.9786751950\n",
      "=> Processing k=200\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 38.6590877270\n",
      "--- Feature extraction elapsed time: 1.3917011950\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 31.4904542450\n",
      "--- Feature extraction elapsed time: 1.3401620940\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 40.0158706880\n",
      "--- Feature extraction elapsed time: 1.3645995210\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 35.2395824810\n",
      "--- Feature extraction elapsed time: 1.3137100320\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 54.5494611020\n",
      "--- Feature extraction elapsed time: 1.3119536510\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 41.6207404640\n",
      "--- Feature extraction elapsed time: 1.3947228390\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 32.5872741050\n",
      "--- Feature extraction elapsed time: 1.3916073190\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 43.2639794630\n",
      "--- Feature extraction elapsed time: 1.9406381470\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 31.6553230960\n",
      "--- Feature extraction elapsed time: 1.2891683120\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 36.9014346030\n",
      "--- Feature extraction elapsed time: 1.4175948190\n",
      "=> Processing k=400\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 44.8195576660\n",
      "--- Feature extraction elapsed time: 2.7694038470\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 57.9321284400\n",
      "--- Feature extraction elapsed time: 3.6475407870\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 50.2541678490\n",
      "--- Feature extraction elapsed time: 2.5023497450\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 44.7358099710\n",
      "--- Feature extraction elapsed time: 2.5832758400\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 51.2910580960\n",
      "--- Feature extraction elapsed time: 3.2535865990\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 50.8863707650\n",
      "--- Feature extraction elapsed time: 2.4908283740\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 57.9859516870\n",
      "--- Feature extraction elapsed time: 3.2184789740\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 51.3749761040\n",
      "--- Feature extraction elapsed time: 2.5522315590\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 56.9503801890\n",
      "--- Feature extraction elapsed time: 2.9448492710\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 51.7647256580\n",
      "--- Feature extraction elapsed time: 2.5743354470\n",
      "=> Processing k=800\n",
      "Processing fold 0\n",
      "--- K-means elapsed time: 85.7183081450\n",
      "--- Feature extraction elapsed time: 6.8218584420\n",
      "Processing fold 1\n",
      "--- K-means elapsed time: 66.6195260750\n",
      "--- Feature extraction elapsed time: 6.8126802290\n",
      "Processing fold 2\n",
      "--- K-means elapsed time: 67.3508246820\n",
      "--- Feature extraction elapsed time: 6.8569860870\n",
      "Processing fold 3\n",
      "--- K-means elapsed time: 67.7041906840\n",
      "--- Feature extraction elapsed time: 6.6540808310\n",
      "Processing fold 4\n",
      "--- K-means elapsed time: 84.2439947520\n",
      "--- Feature extraction elapsed time: 6.7146565080\n",
      "Processing fold 5\n",
      "--- K-means elapsed time: 66.4425864260\n",
      "--- Feature extraction elapsed time: 7.4967029740\n",
      "Processing fold 6\n",
      "--- K-means elapsed time: 84.6915014120\n",
      "--- Feature extraction elapsed time: 6.7104121740\n",
      "Processing fold 7\n",
      "--- K-means elapsed time: 83.2486201090\n",
      "--- Feature extraction elapsed time: 6.3850341180\n",
      "Processing fold 8\n",
      "--- K-means elapsed time: 67.9582485940\n",
      "--- Feature extraction elapsed time: 6.7751448520\n",
      "Processing fold 9\n",
      "--- K-means elapsed time: 66.3642518840\n",
      "--- Feature extraction elapsed time: 7.1254507840\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10 \n",
    "\n",
    "grid_search_metrics = {}\n",
    "grid_search_metrics[\"alpha\"] = []\n",
    "grid_search_metrics[\"K\"] = []\n",
    "grid_search_metrics[\"cv_acc_train\"] = []\n",
    "grid_search_metrics[\"cv_acc_test\"] = []\n",
    "grid_search_metrics[\"cv_MSE_train\"] = []\n",
    "grid_search_metrics[\"cv_MSE_test\"] = []\n",
    "\n",
    "for k in k_list:\n",
    "    print(\"=> Processing k={}\".format(k))\n",
    "#     clusters = k_means(raw_training_data, k)\n",
    "#     codebooks = get_codebooks(clusters)\n",
    "#     selected_features = select_features(raw_training_data, coodebooks)\n",
    "#     features_labels = inject_label(selected_features, one_hot_encoding=True)\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "#         print(\"=====> Processing alpha={}\".format(alpha))\n",
    "        grid_search_metrics[\"alpha\"].append(alpha)\n",
    "        grid_search_metrics[\"K\"].append(k)\n",
    "        \n",
    "        cv_accuracy_train, cv_accuracy_test, cv_MSE_train, cv_MSE_test = k_fold_cv(raw_training_data, k_fold, k, alpha)\n",
    "        \n",
    "        grid_search_metrics[\"cv_acc_train\"].append(cv_accuracy_train)\n",
    "        grid_search_metrics[\"cv_acc_test\"].append(cv_accuracy_test)\n",
    "        grid_search_metrics[\"cv_MSE_train\"].append(cv_MSE_train)\n",
    "        grid_search_metrics[\"cv_MSE_test\"].append(cv_MSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>K</th>\n",
       "      <th>cv_acc_train</th>\n",
       "      <th>cv_acc_test</th>\n",
       "      <th>cv_MSE_train</th>\n",
       "      <th>cv_MSE_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.375235</td>\n",
       "      <td>0.389977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833556</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.397678</td>\n",
       "      <td>0.408196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.839667</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.377793</td>\n",
       "      <td>0.400422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.837667</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.395476</td>\n",
       "      <td>0.408668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.836111</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.384749</td>\n",
       "      <td>0.396459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.385792</td>\n",
       "      <td>0.384418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.830667</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.391607</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.838222</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.384801</td>\n",
       "      <td>0.398354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.397297</td>\n",
       "      <td>0.414552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.817222</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.394618</td>\n",
       "      <td>0.418543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.830667</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.383614</td>\n",
       "      <td>0.393730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.943667</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.232223</td>\n",
       "      <td>0.253311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.945667</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.227121</td>\n",
       "      <td>0.244759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.235754</td>\n",
       "      <td>0.252961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.942889</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.232987</td>\n",
       "      <td>0.252451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.230668</td>\n",
       "      <td>0.249221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.945222</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.230386</td>\n",
       "      <td>0.250584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.939778</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.238167</td>\n",
       "      <td>0.265261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.944778</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.237415</td>\n",
       "      <td>0.254639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.945111</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.231075</td>\n",
       "      <td>0.253332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.223248</td>\n",
       "      <td>0.241333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.227261</td>\n",
       "      <td>0.244472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha   K  cv_acc_train  cv_acc_test  cv_MSE_train  cv_MSE_test\n",
       "0     0.0  10      0.844222        0.847      0.375235     0.389977\n",
       "1     0.1  10      0.833556        0.838      0.397678     0.408196\n",
       "2     0.2  10      0.839667        0.808      0.377793     0.400422\n",
       "3     0.3  10      0.837667        0.826      0.395476     0.408668\n",
       "4     0.4  10      0.836111        0.837      0.384749     0.396459\n",
       "5     0.5  10      0.838333        0.850      0.385792     0.384418\n",
       "6     0.6  10      0.830667        0.811      0.391607     0.424242\n",
       "7     0.7  10      0.838222        0.837      0.384801     0.398354\n",
       "8     0.8  10      0.822000        0.817      0.397297     0.414552\n",
       "9     0.9  10      0.817222        0.799      0.394618     0.418543\n",
       "10    1.0  10      0.830667        0.832      0.383614     0.393730\n",
       "11    0.0  20      0.943667        0.932      0.232223     0.253311\n",
       "12    0.1  20      0.945667        0.941      0.227121     0.244759\n",
       "13    0.2  20      0.943333        0.933      0.235754     0.252961\n",
       "14    0.3  20      0.942889        0.933      0.232987     0.252451\n",
       "15    0.4  20      0.946000        0.942      0.230668     0.249221\n",
       "16    0.5  20      0.945222        0.934      0.230386     0.250584\n",
       "17    0.6  20      0.939778        0.927      0.238167     0.265261\n",
       "18    0.7  20      0.944778        0.926      0.237415     0.254639\n",
       "19    0.8  20      0.945111        0.942      0.231075     0.253332\n",
       "20    0.9  20      0.948000        0.940      0.223248     0.241333\n",
       "21    1.0  20      0.944444        0.941      0.227261     0.244472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>K</th>\n",
       "      <th>cv_acc_train</th>\n",
       "      <th>cv_acc_test</th>\n",
       "      <th>cv_MSE_train</th>\n",
       "      <th>cv_MSE_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363778</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.772451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.663222</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.612294</td>\n",
       "      <td>0.618742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827333</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.402264</td>\n",
       "      <td>0.409435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.227399</td>\n",
       "      <td>0.248641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.967778</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.159679</td>\n",
       "      <td>0.189419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.979667</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.119638</td>\n",
       "      <td>0.157589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.987889</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.084664</td>\n",
       "      <td>0.135771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>400</td>\n",
       "      <td>0.996111</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.049026</td>\n",
       "      <td>0.120286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.105101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha    K  cv_acc_train  cv_acc_test  cv_MSE_train  cv_MSE_test\n",
       "0    0.0    2      0.363778        0.359      0.767940     0.772451\n",
       "1    0.0    5      0.663222        0.647      0.612294     0.618742\n",
       "2    0.0   10      0.827333        0.827      0.402264     0.409435\n",
       "3    0.0   20      0.947222        0.941      0.227399     0.248641\n",
       "4    0.0   50      0.967778        0.965      0.159679     0.189419\n",
       "5    0.0  100      0.979667        0.970      0.119638     0.157589\n",
       "6    0.0  200      0.987889        0.970      0.084664     0.135771\n",
       "7    0.0  400      0.996111        0.976      0.049026     0.120286\n",
       "8    0.0  800      1.000000        0.982      0.006403     0.105101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
